# ğŸ§  MNIST Neural Network Implementation

<div align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=TensorFlow&logoColor=white" />
  <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white" />
  <img src="https://img.shields.io/badge/Matplotlib-11557c?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Accuracy-95.40%25-brightgreen?style=for-the-badge" />
</div>

<div align="center">
  <h3>ğŸ¯ Handwritten Digit Recognition using Custom Neural Networks</h3>
  <p><em>A comprehensive approach to digit classification with both custom and modern implementations</em></p>
</div>

---

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Overview](#-overview)
- [ğŸ“Š Dataset](#-dataset)
- [ğŸ—ï¸ Architecture](#%EF%B8%8F-architecture)
- [âš¡ Features](#-features)
- [ğŸ“ˆ Results](#-results)

---

##  Overview

This project demonstrates a comprehensive approach to handwritten digit recognition using the famous MNIST dataset. It showcases both **custom neural network implementation from scratch** using pure NumPy and modern implementations using TensorFlow/Keras.

### ğŸ¯ **Key Achievements:**
- âœ… **95.40% Accuracy** on MNIST test set
- âœ… Complete implementation from scratch using NumPy
- âœ… Comprehensive data analysis and visualization
- âœ… Modular and educational code structure

---

##  Dataset

The **MNIST dataset** contains 70,000 grayscale images of handwritten digits (0-9):

MNIST Dataset Overview<br>
â”œâ”€â”€ ğŸ‹ï¸ Training Set: 60,000 images<br>
â”œâ”€â”€ ğŸ§ª Test Set: 10,000 images<br> 
â”œâ”€â”€ ğŸ“ Image Size: 28Ã—28 pixels <br>
â”œâ”€â”€ ğŸ¨ Color: Grayscale (0-255) <br>
â””â”€â”€ ğŸ·ï¸ Classes: 10 digits (0-9)<br>

<details>
<summary><b> Dataset Statistics</b></summary>

| Metric | Value |
|--------|-------|
| **Input Dimensions** | 784 features (28Ã—28 flattened) |
| **Output Classes** | 10 digits (0-9) |
| **Data Type** | Grayscale images |
| **Pixel Range** | 0-255 (normalized to 0-1) |
| **Class Distribution** | Relatively balanced |

</details>

---

##  Architecture

### Custom Neural Network
Input Layer (784) â†’ Hidden Layer 1 (128) â†’ Hidden Layer 2 (64) â†’ Output Layer (10)
[ReLU] [ReLU] [Softmax]

<div align="center">

| Layer | Neurons | Activation | Purpose |
|-------|---------|------------|---------|
| Input | 784 | - | Flattened pixel values |
| Hidden 1 | 128 | ReLU | Feature extraction |
| Hidden 2 | 64 | ReLU | Feature refinement |
| Output | 10 | Softmax | Classification probabilities |

</div>

###  Technical Specifications

- **ğŸ”„ Activation Functions:** ReLU (hidden), Softmax (output)
- **ğŸ“‰ Loss Function:** Cross-entropy loss
- **ğŸ¯ Optimizer:** Gradient Descent
- **ğŸ’¾ Implementation:** Pure NumPy with vectorization
- **ğŸƒâ€â™‚ï¸ Training:** 1000 epochs, Learning rate: 0.1

---

## Features

### ğŸ” **Data Analysis & Visualization**
- [x] Sample digit visualization (5Ã—5 grid)
- [x] Digit distribution analysis 
- [x] Average digit heatmap visualization
- [x] Statistical analysis of pixel intensities

### ğŸ§  **Custom Neural Network**
- [x] From-scratch implementation using NumPy
- [x] Forward propagation with vectorized operations
- [x] Backpropagation with gradient computation
- [x] He weight initialization
- [x] Numerical stability optimizations

### ğŸ“Š **Performance Monitoring**
- [x] Real-time cost tracking
- [x] Training progress visualization
- [x] Accuracy monitoring
- [x] Comprehensive evaluation metrics


---

## Results

### ğŸ† **Model Performance**

<div align="center">
  <h3>ğŸ¯ Final Test Accuracy: <span style="color: #4CAF50;">95.40%</span></h3>
</div>

### ğŸ“Š **Training Progress**
ğŸ”„ Training Configuration:<br>
â”œâ”€â”€ ğŸ“š Epochs: 1000<br>
â”œâ”€â”€ ğŸ“ˆ Learning Rate: 0.1 <br>
â””â”€â”€ âš¡ Implementation: Vectorized NumPy<br>

ğŸ“ˆ Training Results:<br>
Epoch Cost Accuracy Progress<br>
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br>
0 2.3896 ~10% â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘<br>
100 0.3730 ~85% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘<br>
200 0.2893 ~88% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘<br>
300 0.2514 ~90% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘<br>
400 0.2261 ~92% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘<br>
500 0.2070 ~93% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘<br>
600 0.1919 ~94% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br>
700 0.1792 ~94.5% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br>
800 0.1681 ~95% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br>
900 0.1585 ~95.4% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br>

### ğŸ“‰ **Cost Reduction**
The model shows excellent convergence with cost decreasing from **2.3896** to **0.1585** over 1000 epochs.
